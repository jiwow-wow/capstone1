# -*- coding: utf-8 -*-
"""cv_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bdyBMimf1r88rbCeVKyLP6qdSBC-17XR
"""

# analyze_image_skin_type.py - íšŒê·€ + ë¶„ë¥˜ ì˜ˆì¸¡ í†µí•© ì¶”ë¡  ì½”ë“œ (log + HuberLoss ëŒ€ì‘)

import subprocess, sys
subprocess.run([sys.executable, "-m", "pip", "install", "mediapipe", "opencv-python"])

from google.colab import drive, files
import torch, os, numpy as np
from torchvision import models, transforms
from PIL import Image, ImageOps
import torch.nn as nn
import cv2
import mediapipe as mp
import matplotlib.pyplot as plt

# âœ… ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ & ì´ë¯¸ì§€ ì—…ë¡œë“œ
drive.mount('/content/drive')
uploaded = files.upload()
image_path = list(uploaded.keys())[0]

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… ê²½ë¡œ ì„¤ì •
regression_ckpt = "/content/drive/MyDrive/CapstoneDesign/NIA/checkpoint/regression/100%/1,2,3"
regression_num_output = [1, 2, 0, 0, 0, 3, 3, 0, 2]
skin_ckpt = "/content/drive/MyDrive/CapstoneDesign/NIA/checkpoint/class/skin_type/state_dict.bin"
skin_label_names = ['ê±´ì„±', 'ë³µí•©ê±´ì„±', 'ë³µí•©ì§€ì„±', 'ì¤‘ì„±', 'ì§€ì„±']

# âœ… transform
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406],
                         [0.229, 0.224, 0.225])
])

# âœ… ë³µì› ê¸°ì¤€
restore_stats = {
    1: {"ìˆ˜ë¶„": (60.6, 10.1), "íƒ„ë ¥": (48.7, 11.9)},
    5: {"ìˆ˜ë¶„": (60.6, 10.1), "íƒ„ë ¥": (48.7, 11.9), "ëª¨ê³µ ê°œìˆ˜": "log"},
    6: {"ìˆ˜ë¶„": (60.2, 9.6),  "íƒ„ë ¥": (49.3, 12.1), "ëª¨ê³µ ê°œìˆ˜": "log"},
    8: {"ìˆ˜ë¶„": (61.3, 10.0), "íƒ„ë ¥": (47.5, 12.0)},
    0: {"ìƒ‰ì†Œì¹¨ì°© ê°œìˆ˜": 300}
}

area_label = {
    0: "ì „ì²´", 1: "ì´ë§ˆ", 2: "ë¯¸ê°„", 3: "ì™¼ìª½ ëˆˆê°€", 4: "ì˜¤ë¥¸ìª½ ëˆˆê°€",
    5: "ì™¼ìª½ ë³¼", 6: "ì˜¤ë¥¸ìª½ ë³¼", 7: "ì…ìˆ ", 8: "í„±"
}
reg_desc = {
    0: ["ìƒ‰ì†Œì¹¨ì°© ê°œìˆ˜"],
    1: ["ìˆ˜ë¶„", "íƒ„ë ¥"],
    5: ["ìˆ˜ë¶„", "íƒ„ë ¥", "ëª¨ê³µ ê°œìˆ˜"],
    6: ["ìˆ˜ë¶„", "íƒ„ë ¥", "ëª¨ê³µ ê°œìˆ˜"],
    8: ["ìˆ˜ë¶„", "íƒ„ë ¥"]
}

# âœ… Mediapipe crop
mp_face_mesh = mp.solutions.face_mesh
REGION_LANDMARKS = {
    0: list(range(468)),
    1: [10, 67, 69, 71, 109, 151, 337, 338, 297],
    2: [168, 6, 197, 195, 5, 4],
    3: [130, 133, 160, 159, 158],
    4: [359, 362, 386, 385, 384],
    5: [205, 50, 187, 201, 213],
    6: [425, 280, 411, 427, 434],
    7: [13, 14, 17, 84, 181],
    8: [152, 377, 400, 378, 379]
}

def crop_regions_by_ratio(pil_img, visualize=False):
    img = np.array(pil_img)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    h, w = img.shape[:2]
    regions = [None] * 9
    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:
        results = face_mesh.process(img_rgb)
        if not results.multi_face_landmarks:
            raise ValueError("â— ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        landmarks = results.multi_face_landmarks[0].landmark
        points = np.array([(lm.x * w, lm.y * h) for lm in landmarks])
        face_x1, face_y1 = np.min(points, axis=0)
        face_x2, face_y2 = np.max(points, axis=0)
        face_w, face_h = face_x2 - face_x1, face_y2 - face_y1
        for idx, lm_indices in REGION_LANDMARKS.items():
            pts = np.array([(landmarks[i].x * w, landmarks[i].y * h) for i in lm_indices])
            cx, cy = np.mean(pts, axis=0)
            if idx == 8: cx -= face_w * 0.15
            if idx == 1:
                box_w, box_h = int(face_w * 0.70), int(face_h * 0.3)
                cy -= box_h * 0.2
            elif idx == 2:
                box_w, box_h = int(face_w * 0.35), int(face_h * 0.15)
                cy -= box_h * 2.5
            else:
                box_w, box_h = int(face_w * 0.28), int(face_h * 0.25)
            x1, y1 = max(int(cx - box_w / 2), 0), max(int(cy - box_h / 2), 0)
            x2, y2 = min(int(cx + box_w / 2), w), min(int(cy + box_h / 2), h)
            crop = img[y1:y2, x1:x2]
            regions[idx] = Image.fromarray(crop)
    if visualize:
        plt.figure(figsize=(12, 4))
        for i in range(1, 9):
            if regions[i]:
                plt.subplot(2, 4, i)
                plt.imshow(regions[i])
                plt.axis('off')
                plt.title(f"Area {i}")
        plt.suptitle("Face Regions")
        plt.show()
    return regions

# âœ… íšŒê·€ ëª¨ë¸ ë¡œë“œ
reg_models = []
for idx, out_dim in enumerate(regression_num_output):
    if out_dim == 0:
        reg_models.append(None)
        continue
    model = models.resnet50(weights=None)
    model.fc = nn.Linear(model.fc.in_features, out_dim)
    ckpt_path = os.path.join(regression_ckpt, str(idx), "state_dict.bin")
    if os.path.exists(ckpt_path):
        state = torch.load(ckpt_path, map_location=device)
        if "model_state" in state:
            state = state["model_state"]
        model.load_state_dict(state, strict=False)
        model.eval()
        reg_models.append(model.to(device))
    else:
        reg_models.append(None)

# âœ… í”¼ë¶€íƒ€ì… ë¶„ë¥˜ ëª¨ë¸
skin_model = models.resnet50(weights=None)
skin_model.fc = nn.Linear(skin_model.fc.in_features, len(skin_label_names))
skin_model.load_state_dict(torch.load(skin_ckpt, map_location=device))
skin_model.eval()
skin_model = skin_model.to(device)

# âœ… ì˜ˆì¸¡ ìˆ˜í–‰
print("\n[ë¶„ì„ ê²°ê³¼ ìš”ì•½]")
image = Image.open(image_path).convert("RGB")
image = ImageOps.exif_transpose(image)
regions = crop_regions_by_ratio(image, visualize=True)
if regions[0] is None:
    print("â— ì „ì²´ ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨: í”¼ë¶€íƒ€ì… ë¶„ë¥˜ ê±´ë„ˆëœ€")

for idx in range(9):
    if reg_models[idx] is None or regions[idx] is None:
        continue
    if idx in [3, 4]:
        continue
    print(f"\nğŸ“ [{area_label[idx]}]")
    crop_tensor = transform(regions[idx]).unsqueeze(0).to(device)
    with torch.no_grad():
        reg_out = reg_models[idx](crop_tensor).squeeze().cpu().numpy()
    if reg_out.ndim == 0:
        reg_out = [reg_out]
    for i, val in enumerate(reg_out):
        label = reg_desc[idx][i]
        if label == "ëª¨ê³µ ê°œìˆ˜" and restore_stats[idx].get(label) == "log":
            val = np.clip(np.exp(val) - 1, 0, 2500)
        elif label in restore_stats[idx] and isinstance(restore_stats[idx][label], tuple):
            mean, std = restore_stats[idx][label]
            val = val * std + mean
        elif label == "ìƒ‰ì†Œì¹¨ì°© ê°œìˆ˜":
            val *= 300
        print(f" - {label}: {round(float(val), 2)}")

# âœ… í”¼ë¶€íƒ€ì… ë¶„ë¥˜ ìˆ˜í–‰
if regions[0] is not None:
    overall_tensor = transform(regions[0]).unsqueeze(0).to(device)
    with torch.no_grad():
        skin_out = skin_model(overall_tensor)
        skin_pred = torch.argmax(skin_out, dim=1).item()
        skin_type = skin_label_names[skin_pred]
    print("ğŸŒ¿ í”¼ë¶€ íƒ€ì… ì˜ˆì¸¡ ê²°ê³¼:", skin_type)
